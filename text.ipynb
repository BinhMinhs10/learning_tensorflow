{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.005047  ,  0.02351889, -0.03519099, -0.0054667 ,  0.00313326],\n",
       "       [-0.01975456, -0.04510192,  0.02065016,  0.0410821 ,  0.00618936],\n",
       "       [ 0.01689341,  0.00601308,  0.0071759 , -0.04490727,  0.02445065]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1, 2, 3]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load(\n",
    "    'imdb_reviews/subwords8k',\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    with_info=True,\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " '. ',\n",
       " 'a_',\n",
       " 'and_',\n",
       " 'of_',\n",
       " 'to_',\n",
       " 's_',\n",
       " 'is_',\n",
       " 'br',\n",
       " 'in_',\n",
       " 'I_',\n",
       " 'that_',\n",
       " 'this_',\n",
       " 'it_',\n",
       " ' /><',\n",
       " ' />',\n",
       " 'was_',\n",
       " 'The_',\n",
       " 'as_']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "encoder.subwords[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_data.shuffle(1000).padded_batch(10)\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3209, 1493,    5, ...,    0,    0,    0],\n",
       "       [7963,   19, 4829, ...,    0,    0,    0],\n",
       "       [ 133, 3306,  124, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 404,   12,   83, ...,    0,    0,    0],\n",
       "       [  62,   27,    9, ...,    0,    0,    0],\n",
       "       [  12,  176, 1037, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch, train_label = next(iter(train_batches))\n",
    "train_batch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          130960    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 131,249\n",
      "Trainable params: 131,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.5391 - accuracy: 0.6721 - val_loss: 0.3822 - val_accuracy: 0.8400\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.3180 - accuracy: 0.8683 - val_loss: 0.3996 - val_accuracy: 0.8400\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.2541 - accuracy: 0.8997 - val_loss: 0.3409 - val_accuracy: 0.8700\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.2211 - accuracy: 0.9168 - val_loss: 0.3524 - val_accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.1941 - accuracy: 0.9301 - val_loss: 0.3353 - val_accuracy: 0.8700\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.1755 - accuracy: 0.9385 - val_loss: 0.4371 - val_accuracy: 0.8700\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.1611 - accuracy: 0.9430 - val_loss: 0.3123 - val_accuracy: 0.8900\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.1470 - accuracy: 0.9476 - val_loss: 0.5209 - val_accuracy: 0.8300\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.1369 - accuracy: 0.9527 - val_loss: 0.5143 - val_accuracy: 0.8500\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 4s 2ms/step - loss: 0.1286 - accuracy: 0.9574 - val_loss: 0.3808 - val_accuracy: 0.8650\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_batches,\n",
    "    epochs=10,\n",
    "    validation_data=test_batches,\n",
    "    validation_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss=history_dict['loss']\n",
    "val_loss=history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8185, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transformer model for language understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(train_examples, val_examples), metadata = tfds.load(\n",
    "    'ted_hrlr_translate/pt_to_en',\n",
    "    with_info=True,\n",
    "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "    as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Custom subword tokenize from training set\n",
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for pt, en in train_examples), target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (pt.numpy() for pt , en in train_examples), target_vocab_size=2**13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799]\n",
      "The origin string: Transformer is awesome\n"
     ]
    }
   ],
   "source": [
    "sample_string = \"Transformer is awesome\"\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print('The origin string: {}'.format(original_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7915 ----> T\n",
      "1248 ----> ran\n",
      "7946 ----> s\n",
      "7194 ----> former \n",
      "13 ----> is \n",
      "2799 ----> awesome\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    "    print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "    lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "    \n",
    "    lang2 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "    lang2.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "    \n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    result_pt.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "    \n",
    "    return result_pt, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                         tf.size(y) <= max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 29), dtype=int64, numpy=\n",
       " array([[8214,  378,    1, ...,    0,    0,    0],\n",
       "        [8214, 1293, 1314, ...,    0,    0,    0],\n",
       "        [8214,   18, 2266, ..., 8055,    2, 8215],\n",
       "        ...,\n",
       "        [8214, 1023,  639, ...,    0,    0,    0],\n",
       "        [8214, 1646,   63, ...,    0,    0,    0],\n",
       "        [8214,   61,  103, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n",
       " array([[8214, 5148, 8068, ...,    0,    0,    0],\n",
       "        [8214, 1307, 7990, ...,    0,    0,    0],\n",
       "        [8214, 3347,   17, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [8214,  203, 2000, ...,    0,    0,    0],\n",
       "        [8214, 1533, 7990, ...,    0,    0,    0],\n",
       "        [8214,  708, 7990, ...,    0,    0,    0]])>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "pt_batch, en_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-nightly\n",
    "!pip install -q tf-models-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official\n",
      "  Downloading tf_models_official-2.2.1-py2.py3-none-any.whl (711 kB)\n",
      "\u001b[K     |████████████████████████████████| 711 kB 932 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (1.4.1)\n",
      "Collecting mlperf-compliance==0.0.10\n",
      "  Downloading mlperf_compliance-0.0.10-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (1.5.6)\n",
      "Requirement already satisfied: gin-config in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (0.3.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (0.8.0)\n",
      "Requirement already satisfied: matplotlib in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (2.1.0)\n",
      "Requirement already satisfied: typing==3.7.4.1 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (3.7.4.1)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (7.0.0)\n",
      "Requirement already satisfied: tensorflow>=2.2.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (2.2.0)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (1.25.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (5.6.3)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.2.1 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (0.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (1.17.2)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (0.25.1)\n",
      "Requirement already satisfied: Pillow in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (6.2.0)\n",
      "Requirement already satisfied: pyyaml in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (5.1.2)\n",
      "Requirement already satisfied: dataclasses in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (0.6)\n",
      "Requirement already satisfied: Cython in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (0.29.13)\n",
      "Requirement already satisfied: oauth2client>=4.1.2 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (4.1.3)\n",
      "Requirement already satisfied: tensorflow-addons in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (0.10.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (1.9.3)\n",
      "Requirement already satisfied: opencv-python-headless in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (4.3.0.36)\n",
      "Requirement already satisfied: six in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (1.15.0)\n",
      "Requirement already satisfied: sentencepiece in /home/user/anaconda3/lib/python3.7/site-packages (from tf-models-official) (0.1.85)\n",
      "Requirement already satisfied: tqdm in /home/user/anaconda3/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (4.36.1)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/user/anaconda3/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (1.24.2)\n",
      "Requirement already satisfied: python-dateutil in /home/user/anaconda3/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (2.8.0)\n",
      "Requirement already satisfied: python-slugify in /home/user/anaconda3/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (4.0.1)\n",
      "Requirement already satisfied: certifi in /home/user/anaconda3/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (2019.9.11)\n",
      "Requirement already satisfied: requests in /home/user/anaconda3/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official) (2.22.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-hub>=0.6.0->tf-models-official) (3.12.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/anaconda3/lib/python3.7/site-packages (from matplotlib->tf-models-official) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/anaconda3/lib/python3.7/site-packages (from matplotlib->tf-models-official) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/user/anaconda3/lib/python3.7/site-packages (from matplotlib->tf-models-official) (2.4.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.21.1)\n",
      "Requirement already satisfied: absl-py in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.9.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (19.2.0)\n",
      "Requirement already satisfied: dill in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.3.1.1)\n",
      "Requirement already satisfied: wrapt in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (1.11.2)\n",
      "Requirement already satisfied: future in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (0.17.1)\n",
      "Requirement already satisfied: termcolor in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (1.1.0)\n",
      "Requirement already satisfied: promise in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (1.27.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (2.2.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (2.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (0.3.3)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (0.33.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (0.1.8)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (3.1.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow>=2.2.0->tf-models-official) (1.6.3)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.9.0 in /home/user/anaconda3/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.18.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.1.0 in /home/user/anaconda3/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.3.0)\n",
      "Requirement already satisfied: google-api-core<2.0dev,>=1.15.0 in /home/user/anaconda3/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (1.21.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /home/user/anaconda3/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official) (0.5.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.2.1->tf-models-official) (0.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/user/anaconda3/lib/python3.7/site-packages (from pandas>=0.22.0->tf-models-official) (2019.3)\n",
      "Requirement already satisfied: httplib2>=0.9.1 in /home/user/anaconda3/lib/python3.7/site-packages (from oauth2client>=4.1.2->tf-models-official) (0.18.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/user/anaconda3/lib/python3.7/site-packages (from oauth2client>=4.1.2->tf-models-official) (0.2.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/user/anaconda3/lib/python3.7/site-packages (from oauth2client>=4.1.2->tf-models-official) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/user/anaconda3/lib/python3.7/site-packages (from oauth2client>=4.1.2->tf-models-official) (4.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-addons->tf-models-official) (2.9.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /home/user/anaconda3/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (3.0.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /home/user/anaconda3/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.0.4)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/user/anaconda3/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/user/anaconda3/lib/python3.7/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/user/anaconda3/lib/python3.7/site-packages (from requests->kaggle>=1.3.9->tf-models-official) (2.8)\n",
      "Requirement already satisfied: setuptools in /home/user/anaconda3/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow-hub>=0.6.0->tf-models-official) (41.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos in /home/user/anaconda3/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow-datasets->tf-models-official) (1.51.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->tf-models-official) (0.16.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->tf-models-official) (1.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->tf-models-official) (3.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/user/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->tf-models-official) (0.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/user/anaconda3/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.9.0->google-cloud-bigquery>=0.31.0->tf-models-official) (4.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/user/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->tf-models-official) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/user/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=2.2.0->tf-models-official) (3.1.0)\n",
      "Installing collected packages: mlperf-compliance, tf-models-official\n",
      "Successfully installed mlperf-compliance-0.0.10 tf-models-official-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from official.modeling import tf_utils\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "import official.nlp.bert.run_classifier\n",
    "import official.nlp.bert.tokenization\n",
    "import official.nlp.data.classifier_data_lib\n",
    "import official.nlp.modeling.losses\n",
    "import official.nlp.modeling.models\n",
    "import official.nlp.modeling.networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_config.json',\n",
       " 'bert_model.ckpt.data-00000-of-00001',\n",
       " 'bert_model.ckpt.index',\n",
       " 'vocab.txt']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_folder_bert = \"gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12\"\n",
    "tf.io.gfile.listdir(gs_folder_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset glue/mrpc/1.0.0 (download: 1.43 MiB, generated: Unknown size, total: 1.43 MiB) to /home/user/tensorflow_datasets/glue/mrpc/1.0.0...\u001b[0m\n",
      "Shuffling and writing examples to /home/user/tensorflow_datasets/glue/mrpc/1.0.0.incompleteTPIYAE/glue-train.tfrecord\n",
      "Shuffling and writing examples to /home/user/tensorflow_datasets/glue/mrpc/1.0.0.incompleteTPIYAE/glue-validation.tfrecord\n",
      "Shuffling and writing examples to /home/user/tensorflow_datasets/glue/mrpc/1.0.0.incompleteTPIYAE/glue-test.tfrecord\n",
      "\u001b[1mDataset glue downloaded and prepared to /home/user/tensorflow_datasets/glue/mrpc/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "glue, info = tfds.load('glue/mrpc', with_info=True,\n",
    "                      batch_size=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'train', 'validation']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glue.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'idx': tf.int32,\n",
       "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "    'sentence1': Text(shape=(), dtype=tf.string),\n",
       "    'sentence2': Text(shape=(), dtype=tf.string),\n",
       "})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not_equivalent', 'equivalent']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features['label'].names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: Tensor(\"strided_slice_2:0\", shape=(), dtype=int32)\n",
      "label: Tensor(\"strided_slice_3:0\", shape=(), dtype=int64)\n",
      "sentence1: Tensor(\"strided_slice_4:0\", shape=(), dtype=string)\n",
      "sentence2: Tensor(\"strided_slice_5:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "glue_train = glue['train']\n",
    "for key, value in glue_train.items():\n",
    "    print(\"{}: {}\".format(key, value[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = bert.tokenization.FullTokenizer(\n",
    "    vocab_file=os.path.join(gs_folder_bert, 'vocab.txt'),\n",
    "    do_lower_case=True\n",
    ")\n",
    "print('Vocab size:', len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'tensor', '##flow', '!']\n",
      "[7592, 23435, 12314, 999]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Hello Tensorflow!\")\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 102]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([\"[CLS]\", \"[SEP]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-3eac6bc3a045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m sentence1 = tf.ragged.constant([\n\u001b[0;32m----> 8\u001b[0;31m     encode_sentence(s) for s in glue_train[\"sentence1\"]])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m sentence2 = tf.ragged.constant([\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdetermined\u001b[0m \u001b[0mat\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m     \u001b[0mexecuting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0myou\u001b[0m \u001b[0mneed\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meither\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0muse\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m     \u001b[0mThe\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcomputed\u001b[0m \u001b[0musing\u001b[0m \u001b[0mshape\u001b[0m \u001b[0minference\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0mregistered\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \"\"\"Returns a `tf.TensorShape` that represents the shape of this tensor.\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0mIn\u001b[0m \u001b[0meager\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0malways\u001b[0m \u001b[0mfully\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mknown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
     ]
    }
   ],
   "source": [
    "def encode_sentence(s):\n",
    "    tokens = list(tokenizer.tokenize(s.numpy()))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "sentence1 = tf.ragged.constant([\n",
    "    encode_sentence(s) for s in glue_train[\"sentence1\"]])\n",
    "\n",
    "sentence2 = tf.ragged.constant([\n",
    "    encode_sentence(s) for s in glue_train[\"sentence2\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
